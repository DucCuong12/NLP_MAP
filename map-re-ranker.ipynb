{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"},{"sourceId":513288,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":406073,"modelId":423996}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:55:34.484779Z","iopub.execute_input":"2025-08-08T15:55:34.485040Z","iopub.status.idle":"2025-08-08T15:55:34.812160Z","shell.execute_reply.started":"2025-08-08T15:55:34.485019Z","shell.execute_reply":"2025-08-08T15:55:34.811370Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:55:36.951965Z","iopub.execute_input":"2025-08-08T15:55:36.952394Z","iopub.status.idle":"2025-08-08T15:55:37.135405Z","shell.execute_reply.started":"2025-08-08T15:55:36.952362Z","shell.execute_reply":"2025-08-08T15:55:37.134296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%dir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= pd.read_csv('../input/map-charting-student-math-misunderstandings/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:55:39.765142Z","iopub.execute_input":"2025-08-08T15:55:39.765488Z","iopub.status.idle":"2025-08-08T15:55:40.008653Z","shell.execute_reply.started":"2025-08-08T15:55:39.765458Z","shell.execute_reply":"2025-08-08T15:55:40.008028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:55:42.002890Z","iopub.execute_input":"2025-08-08T15:55:42.003484Z","iopub.status.idle":"2025-08-08T15:55:42.012521Z","shell.execute_reply.started":"2025-08-08T15:55:42.003459Z","shell.execute_reply":"2025-08-08T15:55:42.011839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"misconceptions = []\nfor i in range(len(df)):\n  if misconceptions.count(df.iloc[i]['Misconception'])==0 and ('Misconception' in df.iloc[i]['Category']) :\n    misconceptions.append(df.iloc[i]['Misconception'])\nprint(len(misconceptions))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:55:43.703109Z","iopub.execute_input":"2025-08-08T15:55:43.703721Z","iopub.status.idle":"2025-08-08T15:55:45.752783Z","shell.execute_reply.started":"2025-08-08T15:55:43.703691Z","shell.execute_reply":"2025-08-08T15:55:45.752141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_with_misconceptions = df[df['Misconception'].isin(misconceptions)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:55:49.050043Z","iopub.execute_input":"2025-08-08T15:55:49.050677Z","iopub.status.idle":"2025-08-08T15:55:49.056887Z","shell.execute_reply.started":"2025-08-08T15:55:49.050653Z","shell.execute_reply":"2025-08-08T15:55:49.056284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(df_with_misconceptions.head(3))\n# print(len(df_with_misconceptions))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_with_misconceptions.to_csv('train_with_misconceptions.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate peft bitsandbytes trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:55:53.082905Z","iopub.execute_input":"2025-08-08T15:55:53.083504Z","iopub.status.idle":"2025-08-08T15:55:56.462675Z","shell.execute_reply.started":"2025-08-08T15:55:53.083471Z","shell.execute_reply":"2025-08-08T15:55:56.461817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorboard","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:56:33.293784Z","iopub.execute_input":"2025-08-08T15:56:33.294843Z","iopub.status.idle":"2025-08-08T15:56:36.393434Z","shell.execute_reply.started":"2025-08-08T15:56:33.294814Z","shell.execute_reply":"2025-08-08T15:56:36.392611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nimport peft\nimport trl\n\nprint(f\"Transformers version: {transformers.__version__}\")\nprint(f\"PEFT version: {peft.__version__}\")\nprint(f\"TRL version: {trl.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:56:17.221248Z","iopub.execute_input":"2025-08-08T15:56:17.221870Z","iopub.status.idle":"2025-08-08T15:56:28.806504Z","shell.execute_reply.started":"2025-08-08T15:56:17.221842Z","shell.execute_reply":"2025-08-08T15:56:28.805806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"CUDA Device Count:\", torch.cuda.device_count())\n\nif torch.cuda.is_available():\n    print(\"GPU Name:\", torch.cuda.get_device_name(0))\nelse:\n    print(\"No GPU detected!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:56:41.430024Z","iopub.execute_input":"2025-08-08T15:56:41.430868Z","iopub.status.idle":"2025-08-08T15:56:42.020022Z","shell.execute_reply.started":"2025-08-08T15:56:41.430837Z","shell.execute_reply":"2025-08-08T15:56:42.019451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_function(examples, tokenizer):\n    prompts = []\n    for q_text, mc_answer, explanation, category, misconception in zip(\n        examples['QuestionText'],\n        examples['MC_Answer'],\n        examples['StudentExplanation'],\n        examples['Category'],\n        examples['Misconception']\n    ):\n        \n        prompt = f\"\"\"<|im_start|>system\nYou are an expert educational psychologist and domain expert. \nGiven a multiple-choice question, the student's chosen answer, and the student's explanation, \nexplain step-by-step (Chain-of-Thought) whether the provided candidate misconception matches the student's explanation.\nShow your reasoning, then give a final answer on the last line as exactly one word: \"Yes\" or \"No\".<|im_end|>\n<|im_start|>user\nQuestion: {q_text}\nStudent's Answer: {mc_answer}\nStudent's Explanation: {explanation}\n\nDoes the explanation above demonstrate the misconception '{misconception}' from the category '{category}'?<|im_end|>\n<|im_start|>assistant\n\"\"\"\n        prompts.append(prompt)\n    \n    \n    model_inputs = tokenizer(prompts, max_length=1024, truncation=True)\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:56:44.906767Z","iopub.execute_input":"2025-08-08T15:56:44.907433Z","iopub.status.idle":"2025-08-08T15:56:44.912846Z","shell.execute_reply.started":"2025-08-08T15:56:44.907405Z","shell.execute_reply":"2025-08-08T15:56:44.912191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclass\nclass ContrastiveDataCollator:\n    tokenizer: AutoTokenizer\n\n    def __call__(self, features: List[Dict[str, any]]) -> Dict[str, any]:\n        batch_size = len(features)\n        \n        \n        if batch_size <= 1:\n            return self.tokenizer.pad(features, padding=True, return_tensors=\"pt\")\n        \n        positive_features = features\n        negative_indices = [(i + np.random.randint(1, batch_size)) % batch_size for i in range(batch_size)]\n        negative_features = [features[i] for i in negative_indices]\n        \n        all_features = positive_features + negative_features\n        \n        padded_batch = self.tokenizer.pad(\n            {\"input_ids\": [f[\"input_ids\"] for f in all_features]},\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        return padded_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:56:51.716551Z","iopub.execute_input":"2025-08-08T15:56:51.717331Z","iopub.status.idle":"2025-08-08T15:56:51.723149Z","shell.execute_reply.started":"2025-08-08T15:56:51.717294Z","shell.execute_reply":"2025-08-08T15:56:51.722300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ContrastiveTrainer(Trainer):\n    def __init__(self, *args, yes_token_id, no_token_id, margin=0.5, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.yes_token_id = yes_token_id\n        self.no_token_id = no_token_id\n        self.margin = margin\n    \n    # override\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        last_token_logits = logits[:, -1, :]\n\n        yes_scores = last_token_logits[:, self.yes_token_id]\n        no_scores = last_token_logits[:, self.no_token_id]\n        \n        scores = yes_scores - no_scores\n\n        # batch=pos+neg\n        batch_size = inputs[\"input_ids\"].shape[0] // 2\n        \n        if batch_size == 0:\n            return super().compute_loss(model, inputs, return_outputs)\n\n        positive_scores = scores[:batch_size]\n        negative_scores = scores[batch_size:]\n\n        # hinge loss\n        losses = torch.clamp(self.margin - positive_scores + negative_scores, min=0)\n        loss = losses.mean()\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:57:00.493111Z","iopub.execute_input":"2025-08-08T15:57:00.493510Z","iopub.status.idle":"2025-08-08T15:57:00.499520Z","shell.execute_reply.started":"2025-08-08T15:57:00.493488Z","shell.execute_reply":"2025-08-08T15:57:00.498777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_ranker(df):\n    # config\n    MODEL_NAME= \"Qwen/Qwen2.5-7B-Instruct\"\n    # TRAIN_CSV_PATH = \"/train_with_misconceptions.csv\" \n    OUTPUT_DIR = \"./qwen2-7b-ranker-finetuned\"\n\n    # load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        \n    # yes,no token id\n    yes_token_id = tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n    no_token_id = tokenizer.encode(\"No\", add_special_tokens=False)[0]\n    print(f\"Token ID for 'Yes': {yes_token_id}, 'No': {no_token_id}\")\n\n    #load dataset\n    \n    # # df = pd.read_csv(TRAIN_CSV_PATH)\n    \n    # df = df_with_misconceptions\n    raw_dataset = Dataset.from_pandas(df)\n\n\n    \n    tokenized_dataset = raw_dataset.map(\n        lambda examples: preprocess_function(examples, tokenizer),\n        batched=True,\n        remove_columns=raw_dataset.column_names # bo cot cu\n    )\n\n\n    \n    # load model with LoRA\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True,\n    )\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n    )\n\n    # config PEFT(LoRA)\n    model = prepare_model_for_kbit_training(model)\n    peft_config = LoraConfig(\n        r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    )\n    model = get_peft_model(model, peft_config)\n    model.config.use_cache = False # use cache\n\n    # conf training\n    training_args = TrainingArguments(\n        output_dir=OUTPUT_DIR,\n        per_device_train_batch_size=2, #batch size=2\n        gradient_accumulation_steps=4, \n        learning_rate=2e-5,\n        num_train_epochs=1,\n        logging_steps=10,\n        save_strategy=\"epoch\",\n        fp16=True,\n        dataloader_num_workers=0,\n        report_to=\"tensorboard\",\n    )\n    \n    # initialize datacollator and trainer\n    data_collator = ContrastiveDataCollator(tokenizer=tokenizer)\n    \n    trainer = ContrastiveTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=data_collator,\n        \n        yes_token_id=yes_token_id,\n        no_token_id=no_token_id,\n    )\n\n    # start_training\n    print(\"Start fine-tuning vá»›i Contrastive Loss...\\n\")\n    trainer.train()\n    print(\"Training completed.\\n\")\n\n    \n    # save model\n    final_adapter_dir = f\"{OUTPUT_DIR}/final_adapters\"\n    trainer.save_model(final_adapter_dir)\n    print(f\"Adapters saved at: {final_adapter_dir}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:57:04.406122Z","iopub.execute_input":"2025-08-08T15:57:04.406406Z","iopub.status.idle":"2025-08-08T15:57:04.414570Z","shell.execute_reply.started":"2025-08-08T15:57:04.406387Z","shell.execute_reply":"2025-08-08T15:57:04.413857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#training\ntrain_ranker(df_with_misconceptions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:57:08.787418Z","iopub.execute_input":"2025-08-08T15:57:08.787705Z","iopub.status.idle":"2025-08-09T00:06:13.126484Z","shell.execute_reply.started":"2025-08-08T15:57:08.787686Z","shell.execute_reply":"2025-08-09T00:06:13.125692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r qwen2_7b_ranker_finetuned.zip /kaggle/working/qwen2-7b-ranker-finetuned/final_adapters\nfrom IPython.display import FileLink\nFileLink('qwen2_7b_ranker_finetuned.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T01:32:39.068652Z","iopub.execute_input":"2025-08-09T01:32:39.069434Z","iopub.status.idle":"2025-08-09T01:32:48.419140Z","shell.execute_reply.started":"2025-08-09T01:32:39.069404Z","shell.execute_reply":"2025-08-09T01:32:48.418205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T00:32:57.600112Z","iopub.execute_input":"2025-08-09T00:32:57.600533Z","iopub.status.idle":"2025-08-09T00:32:57.816712Z","shell.execute_reply.started":"2025-08-09T00:32:57.600502Z","shell.execute_reply":"2025-08-09T00:32:57.815600Z"}},"outputs":[],"execution_count":null}]}