{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # can phai split train / validate\n",
    "positive_df= pd.read_csv('/kaggle/input/train-fixed/train_fixed.csv',keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "negative_df = pd.read_csv('/kaggle/input/negetive-train/negative_train.csv',keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(positive_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(negative_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PEFT version: {peft.__version__}\")\n",
    "print(f\"TRL version: {trl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pandas as pd\n",
    "from datasets import Dataset , load_from_disk\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "# import numpy as np\n",
    "\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples,tokenizer):\n",
    "    tx = tokenizer(\n",
    "        examples['prompt'],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=3000,\n",
    "        return_length=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    return tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#xong\n",
    "def preprocess_function_mini_batch(df):\n",
    "    prompts = []\n",
    "    for q_text, mc_answer, explanation, category, misconception in zip(\n",
    "        df['QuestionText'],\n",
    "        df['MC_Answer'],\n",
    "        df['StudentExplanation'],\n",
    "        df['Category'],\n",
    "        df['Misconception']\n",
    "    ):\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "You are a meticulous educational analyst and expert in mathematics pedagogy. Your task is to perform a verification check. You will be given a student's response to a math problem, and a proposed classification for that response. You must determine if the proposed classification is entirely accurate based on the evidence.\n",
    "DEFINITIONS OF THE CLASSIFICATION LABELS:\n",
    "category: This is a compound label with two parts, separated by an underscore: Correctness_ReasoningType.\n",
    "\n",
    "Part 1: Correctness (True or False): This describes whether the student's mc_answer is objectively the correct solution to the q_text.\n",
    "\n",
    "Part 2: ReasoningType (Correct, Misconception, or Neither): This describes the quality of the student's explanation:\n",
    "Correct: The explanation shows sound, logical, and mathematically valid reasoning.\n",
    "Misconception: The explanation reveals a specific, identifiable error in conceptual understanding.\n",
    "Neither: The explanation is incorrect, but does not point to a specific misconception. It could be a guess, irrelevant, or simply nonsensical.\n",
    "\n",
    "misconception: This is a text description of the specific thinking error. It is only relevant when the ReasoningType in the category is Misconception. If the category is ..._Correct or ..._Neither, this field's value should be \"NA\" or \"nan\" or \"NAN\" ,...\n",
    "\n",
    "YOUR STEP-BY-STEP VERIFICATION PROCESS (Chain-of-Thought):\n",
    "\n",
    "1. Analyze Answer Correctness (True/False Check): First, independently solve the math problem in {q_text}. Compare your result to the student's {mc_answer}. Is the student's answer objectively True (correct) or False (incorrect)?\n",
    "2. Analyze Explanation Quality (Reasoning Check): Now, ignore the final answer and focus only on the {explanation}.\n",
    "Deconstruct the student's logic. What steps did they follow?\n",
    "Based only on their text, classify their reasoning: Is it Correct, a clear Misconception, or Neither?\n",
    "If you identify a misconception, briefly describe it in your own words.\n",
    "3. Compare Your Analysis to the Provided Labels: Now, compare your findings from steps 1 and 2 with the given {category} and {misconception}.\n",
    "Does your True/False conclusion from Step 1 match the first part of the {category} label?\n",
    "Does your Correct/Misconception/Neither conclusion from Step 2 match the second part of the {category} label?\n",
    "If the category is ..._Misconception, does the student's error you identified align with the provided {misconception} text?\n",
    "4. Final Conclusion: A \"Yes\" is only possible if all checks in Step 3 pass. If there is any mismatch at any point, the answer must be \"No\".\n",
    "Show your detailed reasoning by following these steps. Then, on the very last line, provide the final answer as exactly one word: \"Yes\" or \"No\".\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Problem Data:\n",
    "Question: {q_text}\n",
    "Student's Answer: {mc_answer}\n",
    "Student's Explanation: {explanation}\n",
    "Proposed Classification:\n",
    "Category: '{category}'\n",
    "Misconception: '{misconception}'\n",
    "Verification Task:\n",
    "Based on the data and the definitions provided, is the 'Proposed Classification' an accurate description of the 'Problem Data'?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    df['prompt'] = prompts\n",
    "    # model_inputs = tokenizer(prompts, max_length=1024, truncation=True)\n",
    "    # return model_inputs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#chua xong\n",
    "def get_dataset(df,tokenizer,save_to_disk):\n",
    "    df= preprocess_function_mini_batch(df)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    remove_columns = [\"QuestionId\",\"QuestionText\",\"MC_Answer\",\"StudentExplanation\",\"Category\",\"Misconception\",\"prompt\",]\n",
    "    task_dataset = dataset.map(tokenize_function,batched=True,remove_columns=remove_columns,fn_kwargs = {'tokenizer' : tokenizer},batch_size=7000,num_proc=2)\n",
    "    task_dataset.save_to_disk(save_to_disk)\n",
    "    return load_from_disk(save_to_disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# negative_dataset=Dataset.from_pandas(negative_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#xong\n",
    "@dataclass\n",
    "class RankerDataCollator(DataCollatorWithPadding):\n",
    "    tokenizer: AutoTokenizer\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, any]]) -> Dict[str, any]:\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        if batch_size <= 1:\n",
    "            features = {k:v for k,v in feature.items() if k != 'row_id'}\n",
    "            return self.tokenizer.pad(features, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        \n",
    "        \n",
    "        positive_features = list(features)\n",
    "        negative_indices = []\n",
    "        \n",
    "        for feature in postive_features:\n",
    "            idx = feature[\"row_id\"]\n",
    "            offset= idx*73\n",
    "            negative_list = np.random.choice(range(offset+1,offset+73),size=batch_size,replace=False)\n",
    "            negative_list = list(negative_list)\n",
    "            negative_indices += negative_list\n",
    "    \n",
    "        \n",
    "        # negative_indices = [(i + np.random.randint(1, batch_size)) % batch_size for i in range(batch_size)]\n",
    "        negative_features = [negative_dataset[i] for i in negative_indices]\n",
    "        \n",
    "        all_features = positive_features + negative_features\n",
    "        final_features = []\n",
    "\n",
    "        for feature in all_features:\n",
    "            clean = {k:v for k,v in feature.items() if k != 'row_id'}\n",
    "            final_features.append(clean)\n",
    "\n",
    "        \n",
    "        padded_batch = self.tokenizer.pad(\n",
    "            final_features,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "decode_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#xong\n",
    "class CrossEntropyTrainer(Trainer):\n",
    "    def __init__(self, *args, yes_token_id, no_token_id, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.yes_token_id = yes_token_id\n",
    "        self.no_token_id = no_token_id\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # override\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # yes_scores = last_token_logits[:, self.yes_token_id]\n",
    "        # no_scores = last_token_logits[:, self.no_token_id]\n",
    "        # scores = yes_scores - no_scores\n",
    "\n",
    "        \n",
    "        num_positives = self.args.per_device_train_batch_size\n",
    "        num_negatives = num_positives * num_positives\n",
    "        expected_total_size = num_positives + num_negatives\n",
    "\n",
    "        if last_token_logits.shape[0] != expected_total_size:\n",
    "            print(f\"Batch size không khớp! Got {last_token_logits.shape[0]}, expected {expected_total_size}. Skipping loss calculation for this batch.\")\n",
    "            #khong hoc / update weight\n",
    "            return torch.tensor(0.0, device=model.device, requires_grad=True)\n",
    "        \n",
    "        positive_labels = torch.full((num_positives,), self.yes_token_id, dtype=torch.long, device=model.device)\n",
    "        negative_labels = torch.full((num_negatives,), self.no_token_id, dtype=torch.long, device=model.device)\n",
    "        labels = torch.cat([positive_labels, negative_labels])\n",
    "\n",
    "\n",
    "\n",
    "        if decode_steps < 4:\n",
    "        \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            \n",
    "            decoded_inputs = self.tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=False)\n",
    "            decoded_preds = self.tokenizer.decode(predictions[0], skip_special_tokens=False)\n",
    "            \n",
    "            print(\"================= DEBUG=================\")\n",
    "            print(f\"--- INPUT ---:\\n{decoded_inputs}\")\n",
    "            print(f\"--- Response ---:\\n{decoded_preds}\")\n",
    "            print(\"============================================\")\n",
    "            decode_steps+=1\n",
    "\n",
    "\n",
    "\n",
    "        #ce loss\n",
    "        loss = self.loss_fct(last_token_logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME= \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# TRAIN_CSV_PATH = \"/train_with_misconceptions.csv\" \n",
    "OUTPUT_DIR = \"./qwen2-7b-ranker-finetuned\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "positive_dataset=get_dataset(positive_df,tokenizer)\n",
    "negative_dataset=get_dataset(negative_df,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(positive_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(negative_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_ranker():\n",
    "    # config\n",
    "    # MODEL_NAME= \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    # # TRAIN_CSV_PATH = \"/train_with_misconceptions.csv\" \n",
    "    # OUTPUT_DIR = \"./qwen2-7b-ranker-finetuned\"\n",
    "\n",
    "    # # load tokenizer\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    # if tokenizer.pad_token is None:\n",
    "    #     tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    # # yes,no token id\n",
    "    # yes_token_id = tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "    # no_token_id = tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "    # print(f\"Token ID for 'Yes': {yes_token_id}, 'No': {no_token_id}\")\n",
    "\n",
    "    # #load dataset\n",
    "    \n",
    "    # # df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    \n",
    "    # # df = df_with_misconceptions\n",
    "    \n",
    "    # # raw_dataset = Dataset.from_pandas(df)\n",
    "    # # tokenized_dataset = raw_dataset.map(\n",
    "    # #     lambda examples: preprocess_function(examples, tokenizer),\n",
    "    # #     batched=True,\n",
    "    # #     remove_columns=raw_dataset.column_names # bo cot cu\n",
    "    # # )\n",
    "\n",
    "    # # tokenized_dataset = get_dataset(df,tokenizer)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # # load model with LoRA\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True,\n",
    "    # )\n",
    "    \n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n",
    "    # )\n",
    "\n",
    "    # config PEFT(LoRA)\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    peft_config = LoraConfig(\n",
    "        r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.config.use_cache = False # use cache\n",
    "\n",
    "    # conf training\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=2, #batch size=2\n",
    "        gradient_accumulation_steps=4, \n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=5,\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=0,\n",
    "        report_to=\"tensorboard\",\n",
    "    )\n",
    "    \n",
    "    # initialize datacollator and trainer\n",
    "    data_collator = RankerDataCollator(tokenizer=tokenizer)\n",
    "    \n",
    "    trainer = CrossEntropyTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=positive_dataset,\n",
    "        data_collator=data_collator,\n",
    "        \n",
    "        yes_token_id=yes_token_id,\n",
    "        no_token_id=no_token_id,\n",
    "    )\n",
    "\n",
    "    # start_training\n",
    "    print(\"Start fine-tuning với Cross-entropy Loss...\\n\")\n",
    "    trainer.train()\n",
    "    print(\"Training completed.\\n\")\n",
    "\n",
    "    \n",
    "    # save model\n",
    "    final_adapter_dir = f\"{OUTPUT_DIR}/final_adapters\"\n",
    "    trainer.save_model(final_adapter_dir)\n",
    "    print(f\"Adapters saved at: {final_adapter_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "train_ranker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r qwen2_7b_ranker_finetuned.zip /kaggle/working/qwen2-7b-ranker-finetuned/final_adapters\n",
    "from IPython.display import FileLink\n",
    "FileLink('qwen2_7b_ranker_finetuned.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8066283,
     "sourceId": 12760087,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8066292,
     "sourceId": 12760099,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
